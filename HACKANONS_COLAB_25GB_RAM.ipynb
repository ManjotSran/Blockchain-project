{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HACKANONS COLAB 25GB RAM.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManjotSran/Blockchain-project/blob/main/HACKANONS_COLAB_25GB_RAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from csv import reader\n",
        "\n",
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "    file = open(filename, \"rt\")\n",
        "    lines = reader(file)\n",
        "    dataset = list(lines)\n",
        "    return dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm_class, n_folds, max_depth, min_size):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for i in range(n_folds):\n",
        "        train_set = np.concatenate([folds[j] for j in range(n_folds) if j != i])\n",
        "        test_set = np.array(folds[i])\n",
        "\n",
        "        model = algorithm_class(max_depth, min_size)\n",
        "        model.fit(train_set)\n",
        "        predicted = model.predict_dataset(test_set[:, :-1])\n",
        "        actual = test_set[:, -1]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "\n",
        "\n",
        "# Your custom CART implementation\n",
        "def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "    P_L = len(left_y) / len(y)\n",
        "    P_R = 1 - P_L\n",
        "    goodness = 0\n",
        "\n",
        "    for j in range(num_classes):\n",
        "        P_j_tL = np.sum(left_y == j) / len(left_y) if len(left_y) > 0 else 0\n",
        "        P_j_tR = np.sum(right_y == j) / len(right_y) if len(right_y) > 0 else 0\n",
        "        goodness += abs(P_j_tL - P_j_tR)\n",
        "\n",
        "    return 2 * P_L * P_R * goodness\n",
        "\n",
        "def calculate_best_split(dataset, num_features, num_classes):\n",
        "    best_split = {}\n",
        "    max_goodness = -float(\"inf\")\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        feature_values = np.unique(dataset[:, feature_index])\n",
        "        for value in feature_values:\n",
        "            left, right = split_dataset(dataset, feature_index, value)\n",
        "            if len(left) > 0 and len(right) > 0:\n",
        "                y, left_y, right_y = dataset[:, -1], left[:, -1], right[:, -1]\n",
        "                current_goodness = calculate_measure_of_goodness(y, left_y, right_y, num_classes)\n",
        "                if current_goodness > max_goodness:\n",
        "                    best_split[\"feature_index\"] = feature_index\n",
        "                    best_split[\"threshold\"] = value\n",
        "                    best_split[\"left\"] = left\n",
        "                    best_split[\"right\"] = right\n",
        "                    best_split[\"gain\"] = current_goodness\n",
        "                    max_goodness = current_goodness\n",
        "    return best_split\n",
        "\n",
        "def split_dataset(dataset, feature_index, threshold):\n",
        "    left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
        "    right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
        "    return left, right\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, gain=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gain = gain\n",
        "\n",
        "class CART:\n",
        "    def __init__(self, max_depth=10, min_size=2):\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def build_tree(self, dataset, current_depth=0, num_classes=None):\n",
        "        X, y = dataset[:, :-1], dataset[:, -1]\n",
        "        num_features = X.shape[1]\n",
        "        if num_classes is None:\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(set(y)) == 1 or current_depth >= self.max_depth:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Calculate the best split\n",
        "        best_split = calculate_best_split(dataset, num_features, num_classes)\n",
        "        if best_split[\"gain\"] == 0 or len(best_split[\"left\"]) < self.min_size or len(best_split[\"right\"]) < self.min_size:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Build left and right subtrees\n",
        "        left_subtree = self.build_tree(best_split[\"left\"], current_depth + 1, num_classes)\n",
        "        right_subtree = self.build_tree(best_split[\"right\"], current_depth + 1, num_classes)\n",
        "\n",
        "        # Create a tree node\n",
        "        return TreeNode(feature_index=best_split[\"feature_index\"], threshold=best_split[\"threshold\"],\n",
        "                        left=left_subtree, right=right_subtree, gain=best_split[\"gain\"])\n",
        "\n",
        "    def most_common_label(self, y):\n",
        "        return np.bincount(y.astype(int)).argmax()\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        num_classes = len(np.unique(dataset[:, -1]))\n",
        "        self.root = self.build_tree(dataset, num_classes=num_classes)\n",
        "\n",
        "    def predict(self, x, node=None):\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self.predict(x, node.left)\n",
        "        else:\n",
        "            return self.predict(x, node.right)\n",
        "\n",
        "    def predict_dataset(self, X):\n",
        "        return [self.predict(x) for x in X]\n",
        "\n",
        "# Test CART on Bank Note dataset\n",
        "seed(1)\n",
        "filename = 'data_banknote_authentication.csv'\n",
        "dataset = load_csv(filename)\n",
        "\n",
        "# Convert string attributes to integers\n",
        "for i in range(len(dataset[0])):\n",
        "    str_column_to_float(dataset, i)\n",
        "\n",
        "# Convert dataset to numpy array\n",
        "dataset = np.array(dataset, dtype=float)\n",
        "\n",
        "# Evaluate algorithm\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "\n",
        "scores = evaluate_algorithm(dataset, CART, n_folds, max_depth, min_size)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "mjzPmpHL4Qq_",
        "outputId": "d7ef50c9-797b-4a6a-fe5a-ccd601559ea2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data_banknote_authentication.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-de7ec65bc135>\u001b[0m in \u001b[0;36m<cell line: 156>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data_banknote_authentication.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# Convert string attributes to integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-de7ec65bc135>\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_banknote_authentication.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "df = pd.read_csv(filename, header=None)\n",
        "\n",
        "# Inspect the first few rows of the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Inspect data types and missing values\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt1PJpuSX3z2",
        "outputId": "824ec33a-721c-424c-e0b1-187dcae7ee4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0                  1       2           3   4                    5   \\\n",
            "0  39          State-gov   77516   Bachelors  13        Never-married   \n",
            "1  50   Self-emp-not-inc   83311   Bachelors  13   Married-civ-spouse   \n",
            "2  38            Private  215646     HS-grad   9             Divorced   \n",
            "3  53            Private  234721        11th   7   Married-civ-spouse   \n",
            "4  28            Private  338409   Bachelors  13   Married-civ-spouse   \n",
            "\n",
            "                   6               7       8        9     10  11  12  \\\n",
            "0        Adm-clerical   Not-in-family   White     Male  2174   0  40   \n",
            "1     Exec-managerial         Husband   White     Male     0   0  13   \n",
            "2   Handlers-cleaners   Not-in-family   White     Male     0   0  40   \n",
            "3   Handlers-cleaners         Husband   Black     Male     0   0  40   \n",
            "4      Prof-specialty            Wife   Black   Female     0   0  40   \n",
            "\n",
            "               13      14  \n",
            "0   United-States   <=50K  \n",
            "1   United-States   <=50K  \n",
            "2   United-States   <=50K  \n",
            "3   United-States   <=50K  \n",
            "4            Cuba   <=50K  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 32561 entries, 0 to 32560\n",
            "Data columns (total 15 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       32561 non-null  int64 \n",
            " 1   1       32561 non-null  object\n",
            " 2   2       32561 non-null  int64 \n",
            " 3   3       32561 non-null  object\n",
            " 4   4       32561 non-null  int64 \n",
            " 5   5       32561 non-null  object\n",
            " 6   6       32561 non-null  object\n",
            " 7   7       32561 non-null  object\n",
            " 8   8       32561 non-null  object\n",
            " 9   9       32561 non-null  object\n",
            " 10  10      32561 non-null  int64 \n",
            " 11  11      32561 non-null  int64 \n",
            " 12  12      32561 non-null  int64 \n",
            " 13  13      32561 non-null  object\n",
            " 14  14      32561 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 3.7+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Replace '?' with NaN and drop rows with missing values\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encoding categorical features\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == np.object:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Split the data into features and target label\n",
        "X = df.drop(14, axis=1)\n",
        "y = df[14]\n",
        "\n",
        "# Normalize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to numpy array for the model\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Model Evaluation\n",
        "seed(1)\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "\n",
        "scores = evaluate_algorithm(np.column_stack((X, y)), CART, n_folds, max_depth, min_size)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "YsK4QK95YBzq",
        "outputId": "28f9ce8f-109f-4652-c3a6-d698f81eaaad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n",
            "<ipython-input-5-b477ffbfdf79>:16: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if df[column].dtype == np.object:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate_algorithm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b477ffbfdf79>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmin_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scores: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_algorithm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import seed\n",
        "from random import randrange\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "df = pd.read_csv(filename, header=None)\n",
        "\n",
        "# Replace '?' with NaN and drop rows with missing values\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encoding categorical features\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Split the data into features and target label\n",
        "X = df.drop(14, axis=1)\n",
        "y = df[14]\n",
        "\n",
        "# Normalize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to numpy array for the model\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Your custom CART implementation\n",
        "def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "    def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "    P_L = len(left_y) / len(y)\n",
        "    P_R = 1 - P_L\n",
        "    goodness = 0\n",
        "\n",
        "    for j in range(num_classes):\n",
        "        P_j_tL = np.sum(left_y == j) / len(left_y) if len(left_y) > 0 else 0\n",
        "        P_j_tR = np.sum(right_y == j) / len(right_y) if len(right_y) > 0 else 0\n",
        "        goodness += abs(P_j_tL - P_j_tR)\n",
        "\n",
        "    return 2 * P_L * P_R * goodness\n",
        "\n",
        "def calculate_best_split(dataset, num_features, num_classes):\n",
        "    best_split = {}\n",
        "    max_goodness = -float(\"inf\")\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        feature_values = np.unique(dataset[:, feature_index])\n",
        "        for value in feature_values:\n",
        "            left, right = split_dataset(dataset, feature_index, value)\n",
        "            if len(left) > 0 and len(right) > 0:\n",
        "                y, left_y, right_y = dataset[:, -1], left[:, -1], right[:, -1]\n",
        "                current_goodness = calculate_measure_of_goodness(y, left_y, right_y, num_classes)\n",
        "                if current_goodness > max_goodness:\n",
        "                    best_split[\"feature_index\"] = feature_index\n",
        "                    best_split[\"threshold\"] = value\n",
        "                    best_split[\"left\"] = left\n",
        "                    best_split[\"right\"] = right\n",
        "                    best_split[\"gain\"] = current_goodness\n",
        "                    max_goodness = current_goodness\n",
        "    return best_split\n",
        "\n",
        "def split_dataset(dataset, feature_index, threshold):\n",
        "    left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
        "    right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
        "    return left, right\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, gain=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gain = gain\n",
        "\n",
        "class CART:\n",
        "    def __init__(self, max_depth=10, min_size=2):\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def build_tree(self, dataset, current_depth=0, num_classes=None):\n",
        "        X, y = dataset[:, :-1], dataset[:, -1]\n",
        "        num_features = X.shape[1]\n",
        "        if num_classes is None:\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(set(y)) == 1 or current_depth >= self.max_depth:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Calculate the best split\n",
        "        best_split = calculate_best_split(dataset, num_features, num_classes)\n",
        "        if best_split[\"gain\"] == 0 or len(best_split[\"left\"]) < self.min_size or len(best_split[\"right\"]) < self.min_size:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Build left and right subtrees\n",
        "        left_subtree = self.build_tree(best_split[\"left\"], current_depth + 1, num_classes)\n",
        "        right_subtree = self.build_tree(best_split[\"right\"], current_depth + 1, num_classes)\n",
        "\n",
        "        # Create a tree node\n",
        "        return TreeNode(feature_index=best_split[\"feature_index\"], threshold=best_split[\"threshold\"],\n",
        "                        left=left_subtree, right=right_subtree, gain=best_split[\"gain\"])\n",
        "\n",
        "    def most_common_label(self, y):\n",
        "        return np.bincount(y.astype(int)).argmax()\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        num_classes = len(np.unique(dataset[:, -1]))\n",
        "        self.root = self.build_tree(dataset, num_classes=num_classes)\n",
        "\n",
        "    def predict(self, x, node=None):\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self.predict(x, node.left)\n",
        "        else:\n",
        "            return self.predict(x, node.right)\n",
        "\n",
        "    def predict_dataset(self, X):\n",
        "        return [self.predict(x) for x in X]\n",
        "\n",
        "# Other required functions (str_column_to_float, cross_validation_split, accuracy_metric, evaluate_algorithm)\n",
        "# ...\n",
        "\n",
        "# Model Evaluation\n",
        "seed(1)\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "\n",
        "# Combine features and target for the evaluate_algorithm function\n",
        "dataset = np.column_stack((X, y))\n",
        "scores = evaluate_algorithm(dataset, CART, n_folds, max_depth, min_size)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "nKEj_NrfYB2J",
        "outputId": "3824ecee-5d0f-43c9-d1a6-d7b945fc3cdc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 37 (<ipython-input-7-bfaf7d3d2316>, line 38)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-bfaf7d3d2316>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    P_L = len(left_y) / len(y)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import seed\n",
        "from random import randrange\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "df = pd.read_csv(filename, header=None)\n",
        "\n",
        "# Replace '?' with NaN and drop rows with missing values\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encoding categorical features\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Split the data into features and target label\n",
        "X = df.drop(14, axis=1)\n",
        "y = df[14]\n",
        "\n",
        "# Normalize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to numpy array for the model\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Your custom CART implementation\n",
        "def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "    def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "      P_L = len(left_y) / len(y)\n",
        "      P_R = 1 - P_L\n",
        "      goodness = 0\n",
        "\n",
        "    for j in range(num_classes):\n",
        "      P_j_tL = np.sum(left_y == j) / len(left_y) if len(left_y) > 0 else 0\n",
        "      P_j_tR = np.sum(right_y == j) / len(right_y) if len(right_y) > 0 else 0\n",
        "      goodness += abs(P_j_tL - P_j_tR)\n",
        "\n",
        "      return 2 * P_L * P_R * goodness\n",
        "\n",
        "def calculate_best_split(dataset, num_features, num_classes):\n",
        "    best_split = {}\n",
        "    max_goodness = -float(\"inf\")\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        feature_values = np.unique(dataset[:, feature_index])\n",
        "        for value in feature_values:\n",
        "            left, right = split_dataset(dataset, feature_index, value)\n",
        "            if len(left) > 0 and len(right) > 0:\n",
        "                y, left_y, right_y = dataset[:, -1], left[:, -1], right[:, -1]\n",
        "                current_goodness = calculate_measure_of_goodness(y, left_y, right_y, num_classes)\n",
        "                if current_goodness > max_goodness:\n",
        "                    best_split[\"feature_index\"] = feature_index\n",
        "                    best_split[\"threshold\"] = value\n",
        "                    best_split[\"left\"] = left\n",
        "                    best_split[\"right\"] = right\n",
        "                    best_split[\"gain\"] = current_goodness\n",
        "                    max_goodness = current_goodness\n",
        "    return best_split\n",
        "\n",
        "def split_dataset(dataset, feature_index, threshold):\n",
        "    left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
        "    right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
        "    return left, right\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, gain=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gain = gain\n",
        "\n",
        "class CART:\n",
        "    def __init__(self, max_depth=10, min_size=2):\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def build_tree(self, dataset, current_depth=0, num_classes=None):\n",
        "        X, y = dataset[:, :-1], dataset[:, -1]\n",
        "        num_features = X.shape[1]\n",
        "        if num_classes is None:\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(set(y)) == 1 or current_depth >= self.max_depth:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Calculate the best split\n",
        "        best_split = calculate_best_split(dataset, num_features, num_classes)\n",
        "        if best_split[\"gain\"] == 0 or len(best_split[\"left\"]) < self.min_size or len(best_split[\"right\"]) < self.min_size:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Build left and right subtrees\n",
        "        left_subtree = self.build_tree(best_split[\"left\"], current_depth + 1, num_classes)\n",
        "        right_subtree = self.build_tree(best_split[\"right\"], current_depth + 1, num_classes)\n",
        "\n",
        "        # Create a tree node\n",
        "        return TreeNode(feature_index=best_split[\"feature_index\"], threshold=best_split[\"threshold\"],\n",
        "                        left=left_subtree, right=right_subtree, gain=best_split[\"gain\"])\n",
        "\n",
        "    def most_common_label(self, y):\n",
        "        return np.bincount(y.astype(int)).argmax()\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        num_classes = len(np.unique(dataset[:, -1]))\n",
        "        self.root = self.build_tree(dataset, num_classes=num_classes)\n",
        "\n",
        "    def predict(self, x, node=None):\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self.predict(x, node.left)\n",
        "        else:\n",
        "            return self.predict(x, node.right)\n",
        "\n",
        "    def predict_dataset(self, X):\n",
        "        return [self.predict(x) for x in X]\n",
        "\n",
        "# Helper functions\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        row[column] = float(row[column].strip())\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm_class, n_folds, max_depth, min_size):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for i in range(n_folds):\n",
        "        train_set = np.concatenate([folds[j] for j in range(n_folds) if j != i])\n",
        "        test_set = np.array(folds[i])\n",
        "\n",
        "        model = algorithm_class(max_depth, min_size)\n",
        "        model.fit(train_set)\n",
        "        predicted = model.predict_dataset(test_set[:, :-1])\n",
        "        actual = test_set[:, -1]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Model Evaluation\n",
        "seed(1)\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "\n",
        "# Combine features and target for the evaluate_algorithm function\n",
        "dataset = np.column_stack((X, y))\n",
        "scores = evaluate_algorithm(dataset, CART, n_folds, max_depth, min_size)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "9F1b3HT9YB5I",
        "outputId": "d046f9ae-4080-49e4-8f63-0e9c1b310b1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'goodness' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3137ab5dc6e5>\u001b[0m in \u001b[0;36m<cell line: 180>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;31m# Combine features and target for the evaluate_algorithm function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scores: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3137ab5dc6e5>\u001b[0m in \u001b[0;36mevaluate_algorithm\u001b[0;34m(dataset, algorithm_class, n_folds, max_depth, min_size)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3137ab5dc6e5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3137ab5dc6e5>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(self, dataset, current_depth, num_classes)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Calculate the best split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mbest_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gain\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTreeNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3137ab5dc6e5>\u001b[0m in \u001b[0;36mcalculate_best_split\u001b[0;34m(dataset, num_features, num_classes)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mcurrent_goodness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_measure_of_goodness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrent_goodness\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_goodness\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0mbest_split\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"feature_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3137ab5dc6e5>\u001b[0m in \u001b[0;36mcalculate_measure_of_goodness\u001b[0;34m(y, left_y, right_y, num_classes)\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mP_j_tL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_y\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mP_j_tR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_y\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mgoodness\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_j_tL\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mP_j_tR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mP_L\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mP_R\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgoodness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'goodness' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import seed\n",
        "from random import randrange\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "df = pd.read_csv(filename, header=None)\n",
        "\n",
        "# Replace '?' with NaN and drop rows with missing values\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encoding categorical features\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Split the data into features and target label\n",
        "X = df.drop(14, axis=1)\n",
        "y = df[14]\n",
        "\n",
        "# Normalize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to numpy array for the model\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Custom CART implementation\n",
        "def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "    P_L = len(left_y) / len(y)\n",
        "    P_R = 1 - P_L\n",
        "    goodness = 0\n",
        "\n",
        "    for j in range(num_classes):\n",
        "        P_j_tL = np.sum(left_y == j) / len(left_y) if len(left_y) > 0 else 0\n",
        "        P_j_tR = np.sum(right_y == j) / len(right_y) if len(right_y) > 0 else 0\n",
        "        goodness += abs(P_j_tL - P_j_tR)\n",
        "\n",
        "    return 2 * P_L * P_R * goodness\n",
        "\n",
        "def calculate_best_split(dataset, num_features, num_classes):\n",
        "    best_split = {}\n",
        "    max_goodness = -float(\"inf\")\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        feature_values = np.unique(dataset[:, feature_index])\n",
        "        for value in feature_values:\n",
        "            left, right = split_dataset(dataset, feature_index, value)\n",
        "            if len(left) > 0 and len(right) > 0:\n",
        "                y, left_y, right_y = dataset[:, -1], left[:, -1], right[:, -1]\n",
        "                current_goodness = calculate_measure_of_goodness(y, left_y, right_y, num_classes)\n",
        "                if current_goodness > max_goodness:\n",
        "                    best_split[\"feature_index\"] = feature_index\n",
        "                    best_split[\"threshold\"] = value\n",
        "                    best_split[\"left\"] = left\n",
        "                    best_split[\"right\"] = right\n",
        "                    best_split[\"gain\"] = current_goodness\n",
        "                    max_goodness = current_goodness\n",
        "    return best_split\n",
        "\n",
        "def split_dataset(dataset, feature_index, threshold):\n",
        "    left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
        "    right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
        "    return left, right\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, gain=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gain = gain\n",
        "\n",
        "class CART:\n",
        "    def __init__(self, max_depth=10, min_size=2):\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def build_tree(self, dataset, current_depth=0, num_classes=None):\n",
        "        X, y = dataset[:, :-1], dataset[:, -1]\n",
        "        num_features = X.shape[1]\n",
        "        if num_classes is None:\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(set(y)) == 1 or current_depth >= self.max_depth:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Calculate the best split\n",
        "        best_split = calculate_best_split(dataset, num_features, num_classes)\n",
        "        if best_split[\"gain\"] == 0 or len(best_split[\"left\"]) < self.min_size or len(best_split[\"right\"]) < self.min_size:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Build left and right subtrees\n",
        "        left_subtree = self.build_tree(best_split[\"left\"], current_depth + 1, num_classes)\n",
        "        right_subtree = self.build_tree(best_split[\"right\"], current_depth + 1, num_classes)\n",
        "\n",
        "        # Create a tree node\n",
        "        return TreeNode(feature_index=best_split[\"feature_index\"], threshold=best_split[\"threshold\"],\n",
        "                        left=left_subtree, right=right_subtree, gain=best_split[\"gain\"])\n",
        "\n",
        "    def most_common_label(self, y):\n",
        "        return np.bincount(y.astype(int)).argmax()\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        num_classes = len(np.unique(dataset[:, -1]))\n",
        "        self.root = self.build_tree(dataset, num_classes=num_classes)\n",
        "\n",
        "    def predict(self, x, node=None):\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self.predict(x, node.left)\n",
        "        else:\n",
        "            return self.predict(x, node.right)\n",
        "\n",
        "    def predict_dataset(self, X):\n",
        "        return [self.predict(x) for x in X]\n",
        "\n",
        "# Helper functions\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "def evaluate_algorithm(dataset, algorithm_class, n_folds, max_depth, min_size):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for i in range(n_folds):\n",
        "        train_set = np.concatenate([folds[j] for j in range(n_folds) if j != i])\n",
        "        test_set = np.array(folds[i])\n",
        "\n",
        "        model = algorithm_class(max_depth, min_size)\n",
        "        model.fit(train_set)\n",
        "        predicted = model.predict_dataset(test_set[:, :-1])\n",
        "        actual = test_set[:, -1]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "\n",
        "# Model Evaluation\n",
        "seed(1)\n",
        "n_folds = 5\n",
        "max_depth = 5\n",
        "min_size = 10\n",
        "\n",
        "# Combine features and target for the evaluate_algorithm function\n",
        "dataset = np.column_stack((X, y))\n",
        "scores = evaluate_algorithm(dataset, CART, n_folds, max_depth, min_size)\n",
        "\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
      ],
      "metadata": {
        "id": "r7Ql-JwWYB7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UrfH0Z72YB-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mh9feonjYCA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rdUfwi6mYCDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9TICthmYCF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UiXt1ciTYCIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2xo5ejyVYCLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsbfWcsYYCP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUr2ynT9YCTC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}